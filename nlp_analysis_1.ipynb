{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backing off send_request(...) for 2.1s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000155BC7D45E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n",
      "Backing off send_request(...) for 5.0s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000155C16737C0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n",
      "Backing off send_request(...) for 11.1s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000155C1686070>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<module 'utilities._support' from 'c:\\\\Users\\\\tommaso\\\\Documents\\\\_Projects\\\\IGScraper\\\\utilities\\\\_support.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import importlib\n",
    "\n",
    "import spacy\n",
    "\n",
    "from utilities import _support\n",
    "from utilities._support import ig_profiles\n",
    "\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "importlib.reload(_support)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Backing off send_request(...) for 5.5s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000155D28A65B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n"
     ]
    }
   ],
   "source": [
    "with open('utilities/lookup_users.pkl', 'rb') as f:\n",
    "    full_lookup = pickle.load(f)\n",
    "\n",
    "df = pd.read_json('gitshared_csv/matteosalviniofficial/matteosalviniofficial_comments_20230316_0-1100_preprocessed.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div><div id=879d2af7-b388-4b3f-b012-b8232a139a9c style=\"display:none; background-color:#9D6CFF; color:white; width:200px; height:30px; padding-left:5px; border-radius:4px; flex-direction:row; justify-content:space-around; align-items:center;\" onmouseover=\"this.style.backgroundColor='#BA9BF8'\" onmouseout=\"this.style.backgroundColor='#9D6CFF'\" onclick=\"window.commands?.execute('create-mitosheet-from-dataframe-output');\">See Full Dataframe in Mito</div> <script> if (window.commands.hasCommand('create-mitosheet-from-dataframe-output')) document.getElementById('879d2af7-b388-4b3f-b012-b8232a139a9c').style.display = 'flex' </script> <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>user</th>\n",
       "      <th>comment</th>\n",
       "      <th>comment_datetime</th>\n",
       "      <th>comment_likes</th>\n",
       "      <th>comment_prep</th>\n",
       "      <th>comment_token</th>\n",
       "      <th>comment_lemma</th>\n",
       "      <th>comment_postag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ck9MRyyKIT5</td>\n",
       "      <td>user_0000027564</td>\n",
       "      <td>Tutti i cittadini dovrebbero saperlo praticare. Si salverebbero migliaia di vita ogni anno. Questa √® civilt√†, questa √® la societ√† che si evolve. Non con i bagni per chi non √© sicuro del proprio genere...</td>\n",
       "      <td>2022-11-14T21:29:33.000Z</td>\n",
       "      <td>29</td>\n",
       "      <td>tutti i cittadini dovrebbero saperlo praticare. si salverebbero migliaia di vita ogni anno. questa √® civilt√† questa √® la societ√† che si evolve. non con i bagni per chi non √© sicuro del proprio genere.</td>\n",
       "      <td>[[tutti, i, cittadini, dovrebbero, saperlo, praticare], [si, salverebbero, migliaia, di, vita, ogni, anno], [questa, √®, civilt√†, questa, √®, la, societ√†, che, si, evolve], [non, con, i, bagni, per, chi, non, √©, sicuro, del, proprio, genere]]</td>\n",
       "      <td>[[tutto, il, cittadino, dovere, saperlo, praticare], [si, salvare, migliaio, di, vita, ogni, anno], [questo, essere, civilt√†, questo, essere, la, societ√†, che, si, evolvere], [non, con, il, bagno, per, chi, non, √©, sicuro, del, proprio, genera]]</td>\n",
       "      <td>[[T, RD, S, VM, V_PC, V], [PC, V, S, E, S, DI, S], [PD, V, S, PD, V, RD, S, PR, PC, V], [BN, E, RD, S, E, PR, BN, V, A, E_RD, AP, S]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ck9MRyyKIT5</td>\n",
       "      <td>user_0000019631</td>\n",
       "      <td>La Professionalit√† e tempestivit√† di questa ragazza sono encomiabili! Bisognerebbe venissero fatti dei corsi di primo soccorso nelle scuole, sarebbero molto utili!</td>\n",
       "      <td>2022-11-14T21:58:05.000Z</td>\n",
       "      <td>13</td>\n",
       "      <td>la professionalit√† e tempestivit√† di questa ragazza sono encomiabili bisognerebbe venissero fatti dei corsi di primo soccorso nelle scuole sarebbero molto utili</td>\n",
       "      <td>[[la, professionalit√†, e, tempestivit√†, di, questa, ragazza, sono, encomiabili, bisognerebbe, venissero, fatti, dei, corsi, di, primo, soccorso, nelle, scuole, sarebbero, molto, utili]]</td>\n",
       "      <td>[[la, professionalit√†, e, tempestivit√†, di, questo, ragazza, essere, encomiabile, bisognare, venire, fatto, dio, corso, di, primo, soccorso, nella, scuola, essere, molto, utile]]</td>\n",
       "      <td>[[RD, S, CC, S, E, DD, S, V, A, V, VA, V, E_RD, S, E, NO, S, E_RD, S, V, B, A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ck9MRyyKIT5</td>\n",
       "      <td>user_0000030353</td>\n",
       "      <td>Nessuna correlazione con il siero? Fatevi domande gente! Troppi casi in giro!</td>\n",
       "      <td>2022-11-16T07:00:14.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>nessuna correlazione con il siero fatevi domande gente troppi casi in giro</td>\n",
       "      <td>[[nessuna, correlazione, con, il, siero, fatevi, domande, gente, troppi, casi, in, giro]]</td>\n",
       "      <td>[[nessuno, correlazione, con, il, siero, fatevi, domanda, gente, troppo, caso, in, girare]]</td>\n",
       "      <td>[[DI, S, E, RD, S, V, S, S, DI, S, E, S]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ck9MRyyKIT5</td>\n",
       "      <td>user_0000028606</td>\n",
       "      <td>Se stata l‚Äôangelo di quel l ‚Äòuomo üëèüèªüëèüèªüëèüèª</td>\n",
       "      <td>2022-11-14T21:35:28.000Z</td>\n",
       "      <td>2</td>\n",
       "      <td>se stata langelo di quel l uomo</td>\n",
       "      <td>[[se, stata, langelo, di, quel, l, uomo]]</td>\n",
       "      <td>[[se, stare, langelo, di, quello, l, uomo]]</td>\n",
       "      <td>[[CS, V, A, E, PD, RD, S]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ck9MRyyKIT5</td>\n",
       "      <td>user_0000055719</td>\n",
       "      <td>Andrebbe insegnato a scuola</td>\n",
       "      <td>2022-11-15T05:08:19.000Z</td>\n",
       "      <td>1</td>\n",
       "      <td>andrebbe insegnato a scuola</td>\n",
       "      <td>[[andrebbe, insegnato, a, scuola]]</td>\n",
       "      <td>[[andare, insegnare, a, scuola]]</td>\n",
       "      <td>[[VA, V, E, S]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206355</th>\n",
       "      <td>CiW8UfIMwND</td>\n",
       "      <td>user_0000056164</td>\n",
       "      <td>Mitica Oriana!üòç</td>\n",
       "      <td>2022-09-11T08:33:12.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>mitica oriana</td>\n",
       "      <td>[[mitica, oriana]]</td>\n",
       "      <td>[[mitico, oriana]]</td>\n",
       "      <td>[[A, A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206356</th>\n",
       "      <td>CiW8UfIMwND</td>\n",
       "      <td>user_0000010176</td>\n",
       "      <td>üôè‚ù§üôè</td>\n",
       "      <td>2022-09-11T08:13:04.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206357</th>\n",
       "      <td>CiW8UfIMwND</td>\n",
       "      <td>user_0000028852</td>\n",
       "      <td>Esatto üòç</td>\n",
       "      <td>2022-09-11T08:12:32.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>esatto</td>\n",
       "      <td>[[esatto]]</td>\n",
       "      <td>[[esatto]]</td>\n",
       "      <td>[[A]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206358</th>\n",
       "      <td>CiW8UfIMwND</td>\n",
       "      <td>user_0000033569</td>\n",
       "      <td>Matteo, stai raschiando il fondo del barile?</td>\n",
       "      <td>2022-09-11T08:08:17.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td>matteo stai raschiando il fondo del barile</td>\n",
       "      <td>[[matteo, stai, raschiando, il, fondo, del, barile]]</td>\n",
       "      <td>[[matteo, stare, raschiare, il, fondere, del, barile]]</td>\n",
       "      <td>[[S, VA, V, RD, S, E_RD, S]]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206359</th>\n",
       "      <td>CiW8UfIMwND</td>\n",
       "      <td>user_0000022475</td>\n",
       "      <td>üëéü§åüèª</td>\n",
       "      <td>2022-09-11T07:51:12.000Z</td>\n",
       "      <td>0</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table></div>"
      ],
      "text/plain": [
       "            post_id             user  \\\n",
       "0       Ck9MRyyKIT5  user_0000027564   \n",
       "1       Ck9MRyyKIT5  user_0000019631   \n",
       "2       Ck9MRyyKIT5  user_0000030353   \n",
       "3       Ck9MRyyKIT5  user_0000028606   \n",
       "4       Ck9MRyyKIT5  user_0000055719   \n",
       "...             ...              ...   \n",
       "206355  CiW8UfIMwND  user_0000056164   \n",
       "206356  CiW8UfIMwND  user_0000010176   \n",
       "206357  CiW8UfIMwND  user_0000028852   \n",
       "206358  CiW8UfIMwND  user_0000033569   \n",
       "206359  CiW8UfIMwND  user_0000022475   \n",
       "\n",
       "                                                  comment  \\\n",
       "0       Tutti i cittadini dovrebbero saperlo praticare...   \n",
       "1       La Professionalit√† e tempestivit√† di questa ra...   \n",
       "2       Nessuna correlazione con il siero? Fatevi doma...   \n",
       "3                Se stata l‚Äôangelo di quel l ‚Äòuomo üëèüèªüëèüèªüëèüèª   \n",
       "4                             Andrebbe insegnato a scuola   \n",
       "...                                                   ...   \n",
       "206355                                    Mitica Oriana!üòç   \n",
       "206356                                                üôè‚ù§üôè   \n",
       "206357                                           Esatto üòç   \n",
       "206358       Matteo, stai raschiando il fondo del barile?   \n",
       "206359                                                üëéü§åüèª   \n",
       "\n",
       "                comment_datetime  comment_likes  \\\n",
       "0       2022-11-14T21:29:33.000Z             29   \n",
       "1       2022-11-14T21:58:05.000Z             13   \n",
       "2       2022-11-16T07:00:14.000Z              1   \n",
       "3       2022-11-14T21:35:28.000Z              2   \n",
       "4       2022-11-15T05:08:19.000Z              1   \n",
       "...                          ...            ...   \n",
       "206355  2022-09-11T08:33:12.000Z              0   \n",
       "206356  2022-09-11T08:13:04.000Z              0   \n",
       "206357  2022-09-11T08:12:32.000Z              0   \n",
       "206358  2022-09-11T08:08:17.000Z              0   \n",
       "206359  2022-09-11T07:51:12.000Z              0   \n",
       "\n",
       "                                             comment_prep  \\\n",
       "0       tutti i cittadini dovrebbero saperlo praticare...   \n",
       "1       la professionalit√† e tempestivit√† di questa ra...   \n",
       "2       nessuna correlazione con il siero fatevi doman...   \n",
       "3                         se stata langelo di quel l uomo   \n",
       "4                             andrebbe insegnato a scuola   \n",
       "...                                                   ...   \n",
       "206355                                      mitica oriana   \n",
       "206356                                                      \n",
       "206357                                             esatto   \n",
       "206358         matteo stai raschiando il fondo del barile   \n",
       "206359                                                      \n",
       "\n",
       "                                            comment_token  \\\n",
       "0       [[tutti, i, cittadini, dovrebbero, saperlo, pr...   \n",
       "1       [[la, professionalit√†, e, tempestivit√†, di, qu...   \n",
       "2       [[nessuna, correlazione, con, il, siero, fatev...   \n",
       "3               [[se, stata, langelo, di, quel, l, uomo]]   \n",
       "4                      [[andrebbe, insegnato, a, scuola]]   \n",
       "...                                                   ...   \n",
       "206355                                 [[mitica, oriana]]   \n",
       "206356                                                 []   \n",
       "206357                                         [[esatto]]   \n",
       "206358  [[matteo, stai, raschiando, il, fondo, del, ba...   \n",
       "206359                                                 []   \n",
       "\n",
       "                                            comment_lemma  \\\n",
       "0       [[tutto, il, cittadino, dovere, saperlo, prati...   \n",
       "1       [[la, professionalit√†, e, tempestivit√†, di, qu...   \n",
       "2       [[nessuno, correlazione, con, il, siero, fatev...   \n",
       "3             [[se, stare, langelo, di, quello, l, uomo]]   \n",
       "4                        [[andare, insegnare, a, scuola]]   \n",
       "...                                                   ...   \n",
       "206355                                 [[mitico, oriana]]   \n",
       "206356                                                 []   \n",
       "206357                                         [[esatto]]   \n",
       "206358  [[matteo, stare, raschiare, il, fondere, del, ...   \n",
       "206359                                                 []   \n",
       "\n",
       "                                           comment_postag  \n",
       "0       [[T, RD, S, VM, V_PC, V], [PC, V, S, E, S, DI,...  \n",
       "1       [[RD, S, CC, S, E, DD, S, V, A, V, VA, V, E_RD...  \n",
       "2               [[DI, S, E, RD, S, V, S, S, DI, S, E, S]]  \n",
       "3                              [[CS, V, A, E, PD, RD, S]]  \n",
       "4                                         [[VA, V, E, S]]  \n",
       "...                                                   ...  \n",
       "206355                                           [[A, A]]  \n",
       "206356                                                 []  \n",
       "206357                                              [[A]]  \n",
       "206358                       [[S, VA, V, RD, S, E_RD, S]]  \n",
       "206359                                                 []  \n",
       "\n",
       "[206360 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as scipy\n",
    "\n",
    "from gensim.models import Word2Vec, LdaModel\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.corpora import Dictionary\n",
    "\n",
    "\"\"\"\n",
    "Word 2 Vec\n",
    "\"\"\"\n",
    "\n",
    "def w2v(sentences, model='SG', vector_size=300, min_count=10, get_bigrams=True, min_count_bigrams=10):\n",
    "    if get_bigrams:\n",
    "        sentences = generate_bi_grams(sentences, min_count=min_count_bigrams, threshold=0.5, progress_per=1000, scoring='npmi')\n",
    "\n",
    "    w2v_model = Word2Vec(sentences, min_count=min_count, vector_size=vector_size, sg=(0 if model=='CBOW' else 1))\n",
    "    return w2v_model\n",
    "\n",
    "def get_w2v_vocab(w2v_model):\n",
    "    return w2v_model.wv.key_to_index\n",
    "\n",
    "def generate_bi_grams(sentences, **kwargs):\n",
    "    phrases = Phrases(sentences, **kwargs)\n",
    "    phrases_model = Phraser(phrases)\n",
    "    return [phrases_model[sent] for sent in sentences]\n",
    "\n",
    "# --------------------------------------------------------------------------- #\n",
    "\n",
    "\"\"\" Word embeddings utils \"\"\"\n",
    "\n",
    "def wvec(wv_model, w):\n",
    "    return wv_model.wv[w]\n",
    "\n",
    "def wnorm(wv_model, w):\n",
    "    vec = wv_model.wv[w] if type(w) is str else w\n",
    "    return math.sqrt(sum([math.pow(v,2) for v in vec]))\n",
    "\n",
    "def wsim(wv_model, pos, neg=[], topn=10, thresh=0, comp=-1):\n",
    "    pos = [pos] if type(pos) is str else pos\n",
    "    neg = [neg] if type(neg) is str else neg\n",
    "    sim0 = list(wv_model.wv.most_similar(positive=pos, negative=neg, topn=topn))\n",
    "    if thresh > 0:\n",
    "        sim0 = [s for s in sim0 if s[1] >= thresh]\n",
    "    if comp != -1:\n",
    "        sim0 = [s[0] if comp==0 else s[1] for s in sim0]\n",
    "    return sim0\n",
    "\n",
    "def vsim(wv_model, vector, topn=10, comp=-1):\n",
    "    sim = wv_model.wv.similar_by_vector(vector, topn=topn)\n",
    "    if comp == 0:\n",
    "        return [s[0] for s in sim]\n",
    "    elif comp == 1:\n",
    "        return [s[1] for s in sim]  \n",
    "    else:\n",
    "        return sim\n",
    "\n",
    "def wanal(wv_model, str_anal, topn=10, thresh=0):\n",
    "    p1 = str_anal[:str_anal.index('=')]\n",
    "    p2 = str_anal[str_anal.index('=')+1:]\n",
    "    neg = p1[:p1.index(':')].replace(' ', '')\n",
    "    pos1 = p1[p1.index(':')+1:].replace(' ', '')\n",
    "    pos2 = p2[:p2.index(':')].replace(' ', '')\n",
    "    return wsim(wv_model, [pos1, pos2], neg, topn=topn, thresh=thresh)\n",
    "\n",
    "def wdist(wv_model, w1, w2):\n",
    "    w1 = wv_model.wv[w1] if type(w1) is str else w1\n",
    "    w2 = wv_model.wv[w2] if type(w2) is str else w2\n",
    "    return 1-scipy.spatial.distance.cosine(w1, w2)\n",
    "\n",
    "def nwdist(wv_model, ws1, ws2):\n",
    "    ws1 = [w for w in ws1 if w in wv_model.wv.key_to_index]\n",
    "    ws2 = [w for w in ws2 if w in wv_model.wv.key_to_index]\n",
    "    return wv_model.wv.n_similarity(ws1,ws2)\n",
    "\n",
    "def wchoesion(wv_model, words):\n",
    "    wsum = 0\n",
    "    for w1 in words:\n",
    "        for w2 in words:\n",
    "            wsum = wsum + ((1-wdist(wv_model, w1, w2)) if w1!=w2 else 0)\n",
    "    return (wsum / (math.pow(len(words),2)-len(words))) if (math.pow(len(words),2)-len(words))!=0 else 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v needs list of sentences: list[list[string]]\n",
    "\n",
    "sentences = [sentence for comment in df.comment_lemma for sentence in comment if sentence != [] and sentence!=None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "PROGRESS: at sentence #1000, processed 6496 words and 5839 word types\n",
      "PROGRESS: at sentence #2000, processed 14202 words and 11309 word types\n",
      "PROGRESS: at sentence #3000, processed 21540 words and 15855 word types\n",
      "PROGRESS: at sentence #4000, processed 30380 words and 20984 word types\n",
      "PROGRESS: at sentence #5000, processed 38575 words and 25496 word types\n",
      "PROGRESS: at sentence #6000, processed 46117 words and 29603 word types\n",
      "PROGRESS: at sentence #7000, processed 53913 words and 33158 word types\n",
      "PROGRESS: at sentence #8000, processed 62003 words and 37024 word types\n",
      "PROGRESS: at sentence #9000, processed 68729 words and 40099 word types\n",
      "PROGRESS: at sentence #10000, processed 76176 words and 43386 word types\n",
      "PROGRESS: at sentence #11000, processed 84821 words and 47217 word types\n",
      "PROGRESS: at sentence #12000, processed 91576 words and 49886 word types\n",
      "PROGRESS: at sentence #13000, processed 99658 words and 53467 word types\n",
      "PROGRESS: at sentence #14000, processed 106907 words and 56693 word types\n",
      "PROGRESS: at sentence #15000, processed 114875 words and 59858 word types\n",
      "PROGRESS: at sentence #16000, processed 122754 words and 63185 word types\n",
      "PROGRESS: at sentence #17000, processed 130411 words and 66546 word types\n",
      "PROGRESS: at sentence #18000, processed 138663 words and 69751 word types\n",
      "PROGRESS: at sentence #19000, processed 145651 words and 72321 word types\n",
      "PROGRESS: at sentence #20000, processed 153069 words and 75054 word types\n",
      "PROGRESS: at sentence #21000, processed 161652 words and 78320 word types\n",
      "PROGRESS: at sentence #22000, processed 169749 words and 81250 word types\n",
      "PROGRESS: at sentence #23000, processed 178119 words and 84295 word types\n",
      "PROGRESS: at sentence #24000, processed 185829 words and 87125 word types\n",
      "PROGRESS: at sentence #25000, processed 193097 words and 89752 word types\n",
      "PROGRESS: at sentence #26000, processed 199823 words and 92056 word types\n",
      "PROGRESS: at sentence #27000, processed 207653 words and 95033 word types\n",
      "PROGRESS: at sentence #28000, processed 214375 words and 97284 word types\n",
      "PROGRESS: at sentence #29000, processed 222414 words and 100169 word types\n",
      "PROGRESS: at sentence #30000, processed 231677 words and 103525 word types\n",
      "PROGRESS: at sentence #31000, processed 239569 words and 106469 word types\n",
      "PROGRESS: at sentence #32000, processed 248063 words and 109467 word types\n",
      "PROGRESS: at sentence #33000, processed 256066 words and 112033 word types\n",
      "PROGRESS: at sentence #34000, processed 264115 words and 114438 word types\n",
      "PROGRESS: at sentence #35000, processed 272950 words and 117313 word types\n",
      "PROGRESS: at sentence #36000, processed 281903 words and 119924 word types\n",
      "PROGRESS: at sentence #37000, processed 289038 words and 122300 word types\n",
      "PROGRESS: at sentence #38000, processed 296969 words and 124829 word types\n",
      "PROGRESS: at sentence #39000, processed 304133 words and 126958 word types\n",
      "PROGRESS: at sentence #40000, processed 313126 words and 129713 word types\n",
      "PROGRESS: at sentence #41000, processed 321317 words and 131325 word types\n",
      "PROGRESS: at sentence #42000, processed 328767 words and 133638 word types\n",
      "PROGRESS: at sentence #43000, processed 336624 words and 135928 word types\n",
      "PROGRESS: at sentence #44000, processed 343748 words and 137914 word types\n",
      "PROGRESS: at sentence #45000, processed 351829 words and 140556 word types\n",
      "PROGRESS: at sentence #46000, processed 358752 words and 142645 word types\n",
      "PROGRESS: at sentence #47000, processed 368430 words and 145585 word types\n",
      "PROGRESS: at sentence #48000, processed 375400 words and 147531 word types\n",
      "PROGRESS: at sentence #49000, processed 383566 words and 149769 word types\n",
      "PROGRESS: at sentence #50000, processed 391032 words and 151861 word types\n",
      "PROGRESS: at sentence #51000, processed 398316 words and 154011 word types\n",
      "PROGRESS: at sentence #52000, processed 405771 words and 156132 word types\n",
      "PROGRESS: at sentence #53000, processed 413084 words and 158065 word types\n",
      "PROGRESS: at sentence #54000, processed 420102 words and 160036 word types\n",
      "PROGRESS: at sentence #55000, processed 427912 words and 162319 word types\n",
      "PROGRESS: at sentence #56000, processed 435377 words and 164459 word types\n",
      "PROGRESS: at sentence #57000, processed 443497 words and 166687 word types\n",
      "PROGRESS: at sentence #58000, processed 451118 words and 168564 word types\n",
      "PROGRESS: at sentence #59000, processed 458266 words and 170261 word types\n",
      "PROGRESS: at sentence #60000, processed 464909 words and 172062 word types\n",
      "PROGRESS: at sentence #61000, processed 472198 words and 174185 word types\n",
      "PROGRESS: at sentence #62000, processed 481365 words and 176902 word types\n",
      "PROGRESS: at sentence #63000, processed 488790 words and 178940 word types\n",
      "PROGRESS: at sentence #64000, processed 496039 words and 180772 word types\n",
      "PROGRESS: at sentence #65000, processed 503207 words and 182786 word types\n",
      "PROGRESS: at sentence #66000, processed 510513 words and 184871 word types\n",
      "PROGRESS: at sentence #67000, processed 518325 words and 187171 word types\n",
      "PROGRESS: at sentence #68000, processed 526668 words and 189636 word types\n",
      "PROGRESS: at sentence #69000, processed 533337 words and 191582 word types\n",
      "PROGRESS: at sentence #70000, processed 541102 words and 193850 word types\n",
      "PROGRESS: at sentence #71000, processed 547206 words and 195569 word types\n",
      "PROGRESS: at sentence #72000, processed 555862 words and 197827 word types\n",
      "PROGRESS: at sentence #73000, processed 563126 words and 199790 word types\n",
      "PROGRESS: at sentence #74000, processed 570882 words and 202054 word types\n",
      "PROGRESS: at sentence #75000, processed 578139 words and 203974 word types\n",
      "PROGRESS: at sentence #76000, processed 586120 words and 205991 word types\n",
      "PROGRESS: at sentence #77000, processed 594389 words and 208278 word types\n",
      "PROGRESS: at sentence #78000, processed 602695 words and 210464 word types\n",
      "PROGRESS: at sentence #79000, processed 610832 words and 212845 word types\n",
      "PROGRESS: at sentence #80000, processed 617507 words and 214549 word types\n",
      "PROGRESS: at sentence #81000, processed 625550 words and 216511 word types\n",
      "PROGRESS: at sentence #82000, processed 631946 words and 218135 word types\n",
      "PROGRESS: at sentence #83000, processed 639461 words and 219889 word types\n",
      "PROGRESS: at sentence #84000, processed 647063 words and 221741 word types\n",
      "PROGRESS: at sentence #85000, processed 655415 words and 223983 word types\n",
      "PROGRESS: at sentence #86000, processed 664849 words and 226426 word types\n",
      "PROGRESS: at sentence #87000, processed 672712 words and 228312 word types\n",
      "PROGRESS: at sentence #88000, processed 680741 words and 230393 word types\n",
      "PROGRESS: at sentence #89000, processed 688500 words and 232293 word types\n",
      "PROGRESS: at sentence #90000, processed 697362 words and 234326 word types\n",
      "PROGRESS: at sentence #91000, processed 705826 words and 236480 word types\n",
      "PROGRESS: at sentence #92000, processed 713759 words and 238358 word types\n",
      "PROGRESS: at sentence #93000, processed 722292 words and 240547 word types\n",
      "PROGRESS: at sentence #94000, processed 730802 words and 242558 word types\n",
      "PROGRESS: at sentence #95000, processed 737997 words and 244246 word types\n",
      "PROGRESS: at sentence #96000, processed 746361 words and 246253 word types\n",
      "PROGRESS: at sentence #97000, processed 753241 words and 247963 word types\n",
      "PROGRESS: at sentence #98000, processed 761236 words and 249859 word types\n",
      "PROGRESS: at sentence #99000, processed 769664 words and 251994 word types\n",
      "PROGRESS: at sentence #100000, processed 777437 words and 254047 word types\n",
      "PROGRESS: at sentence #101000, processed 785683 words and 256037 word types\n",
      "PROGRESS: at sentence #102000, processed 794123 words and 258144 word types\n",
      "PROGRESS: at sentence #103000, processed 801801 words and 260110 word types\n",
      "PROGRESS: at sentence #104000, processed 809982 words and 262250 word types\n",
      "PROGRESS: at sentence #105000, processed 818340 words and 264525 word types\n",
      "PROGRESS: at sentence #106000, processed 827579 words and 266739 word types\n",
      "PROGRESS: at sentence #107000, processed 836080 words and 268751 word types\n",
      "PROGRESS: at sentence #108000, processed 844533 words and 270855 word types\n",
      "PROGRESS: at sentence #109000, processed 851315 words and 272502 word types\n",
      "PROGRESS: at sentence #110000, processed 858187 words and 274148 word types\n",
      "PROGRESS: at sentence #111000, processed 866346 words and 275898 word types\n",
      "PROGRESS: at sentence #112000, processed 875039 words and 278009 word types\n",
      "PROGRESS: at sentence #113000, processed 883384 words and 279794 word types\n",
      "PROGRESS: at sentence #114000, processed 891904 words and 282124 word types\n",
      "PROGRESS: at sentence #115000, processed 901253 words and 284100 word types\n",
      "PROGRESS: at sentence #116000, processed 910083 words and 285994 word types\n",
      "PROGRESS: at sentence #117000, processed 920213 words and 288228 word types\n",
      "PROGRESS: at sentence #118000, processed 928862 words and 290123 word types\n",
      "PROGRESS: at sentence #119000, processed 937161 words and 291926 word types\n",
      "PROGRESS: at sentence #120000, processed 945707 words and 293834 word types\n",
      "PROGRESS: at sentence #121000, processed 954582 words and 295731 word types\n",
      "PROGRESS: at sentence #122000, processed 962951 words and 297638 word types\n",
      "PROGRESS: at sentence #123000, processed 970482 words and 299286 word types\n",
      "PROGRESS: at sentence #124000, processed 978716 words and 301160 word types\n",
      "PROGRESS: at sentence #125000, processed 986221 words and 302821 word types\n",
      "PROGRESS: at sentence #126000, processed 994788 words and 304637 word types\n",
      "PROGRESS: at sentence #127000, processed 1002829 words and 306462 word types\n",
      "PROGRESS: at sentence #128000, processed 1010295 words and 307996 word types\n",
      "PROGRESS: at sentence #129000, processed 1018388 words and 309891 word types\n",
      "PROGRESS: at sentence #130000, processed 1027473 words and 311924 word types\n",
      "PROGRESS: at sentence #131000, processed 1037043 words and 314227 word types\n",
      "PROGRESS: at sentence #132000, processed 1046019 words and 316308 word types\n",
      "PROGRESS: at sentence #133000, processed 1053329 words and 318093 word types\n",
      "PROGRESS: at sentence #134000, processed 1061123 words and 319942 word types\n",
      "PROGRESS: at sentence #135000, processed 1067507 words and 321102 word types\n",
      "PROGRESS: at sentence #136000, processed 1075695 words and 322813 word types\n",
      "PROGRESS: at sentence #137000, processed 1083501 words and 324735 word types\n",
      "PROGRESS: at sentence #138000, processed 1090507 words and 326394 word types\n",
      "PROGRESS: at sentence #139000, processed 1097962 words and 327850 word types\n",
      "PROGRESS: at sentence #140000, processed 1104872 words and 329178 word types\n",
      "PROGRESS: at sentence #141000, processed 1112928 words and 330911 word types\n",
      "PROGRESS: at sentence #142000, processed 1120560 words and 332479 word types\n",
      "PROGRESS: at sentence #143000, processed 1127568 words and 333782 word types\n",
      "PROGRESS: at sentence #144000, processed 1137243 words and 335916 word types\n",
      "PROGRESS: at sentence #145000, processed 1146087 words and 337721 word types\n",
      "PROGRESS: at sentence #146000, processed 1154991 words and 339522 word types\n",
      "PROGRESS: at sentence #147000, processed 1163404 words and 341314 word types\n",
      "PROGRESS: at sentence #148000, processed 1171721 words and 343055 word types\n",
      "PROGRESS: at sentence #149000, processed 1180671 words and 345338 word types\n",
      "PROGRESS: at sentence #150000, processed 1189096 words and 347118 word types\n",
      "PROGRESS: at sentence #151000, processed 1197410 words and 348978 word types\n",
      "PROGRESS: at sentence #152000, processed 1205142 words and 350609 word types\n",
      "PROGRESS: at sentence #153000, processed 1213920 words and 352602 word types\n",
      "PROGRESS: at sentence #154000, processed 1221694 words and 354159 word types\n",
      "PROGRESS: at sentence #155000, processed 1229914 words and 356023 word types\n",
      "PROGRESS: at sentence #156000, processed 1238630 words and 358051 word types\n",
      "PROGRESS: at sentence #157000, processed 1247711 words and 360004 word types\n",
      "PROGRESS: at sentence #158000, processed 1255382 words and 361351 word types\n",
      "PROGRESS: at sentence #159000, processed 1261697 words and 362660 word types\n",
      "PROGRESS: at sentence #160000, processed 1268756 words and 364034 word types\n",
      "PROGRESS: at sentence #161000, processed 1276687 words and 365782 word types\n",
      "PROGRESS: at sentence #162000, processed 1284390 words and 367514 word types\n",
      "PROGRESS: at sentence #163000, processed 1291799 words and 369289 word types\n",
      "PROGRESS: at sentence #164000, processed 1300042 words and 371078 word types\n",
      "PROGRESS: at sentence #165000, processed 1306115 words and 372341 word types\n",
      "PROGRESS: at sentence #166000, processed 1313891 words and 373776 word types\n",
      "PROGRESS: at sentence #167000, processed 1321183 words and 375346 word types\n",
      "PROGRESS: at sentence #168000, processed 1328869 words and 377012 word types\n",
      "PROGRESS: at sentence #169000, processed 1335741 words and 378533 word types\n",
      "PROGRESS: at sentence #170000, processed 1342362 words and 379814 word types\n",
      "PROGRESS: at sentence #171000, processed 1350122 words and 381215 word types\n",
      "PROGRESS: at sentence #172000, processed 1357446 words and 382895 word types\n",
      "PROGRESS: at sentence #173000, processed 1365444 words and 384569 word types\n",
      "PROGRESS: at sentence #174000, processed 1372725 words and 385919 word types\n",
      "PROGRESS: at sentence #175000, processed 1379420 words and 387477 word types\n",
      "PROGRESS: at sentence #176000, processed 1385935 words and 388846 word types\n",
      "PROGRESS: at sentence #177000, processed 1393609 words and 390332 word types\n",
      "PROGRESS: at sentence #178000, processed 1401551 words and 391744 word types\n",
      "PROGRESS: at sentence #179000, processed 1410365 words and 393415 word types\n",
      "PROGRESS: at sentence #180000, processed 1418440 words and 394843 word types\n",
      "PROGRESS: at sentence #181000, processed 1426190 words and 396406 word types\n",
      "PROGRESS: at sentence #182000, processed 1432944 words and 397659 word types\n",
      "PROGRESS: at sentence #183000, processed 1439530 words and 399010 word types\n",
      "PROGRESS: at sentence #184000, processed 1446381 words and 400453 word types\n",
      "PROGRESS: at sentence #185000, processed 1453863 words and 402034 word types\n",
      "PROGRESS: at sentence #186000, processed 1460420 words and 403218 word types\n",
      "PROGRESS: at sentence #187000, processed 1467191 words and 404573 word types\n",
      "PROGRESS: at sentence #188000, processed 1474770 words and 406020 word types\n",
      "PROGRESS: at sentence #189000, processed 1480065 words and 407096 word types\n",
      "PROGRESS: at sentence #190000, processed 1486251 words and 408490 word types\n",
      "PROGRESS: at sentence #191000, processed 1492900 words and 409842 word types\n",
      "PROGRESS: at sentence #192000, processed 1500105 words and 411349 word types\n",
      "PROGRESS: at sentence #193000, processed 1506466 words and 412620 word types\n",
      "PROGRESS: at sentence #194000, processed 1514098 words and 414213 word types\n",
      "PROGRESS: at sentence #195000, processed 1521826 words and 415882 word types\n",
      "PROGRESS: at sentence #196000, processed 1528820 words and 417289 word types\n",
      "PROGRESS: at sentence #197000, processed 1536527 words and 418922 word types\n",
      "PROGRESS: at sentence #198000, processed 1544193 words and 420596 word types\n",
      "PROGRESS: at sentence #199000, processed 1550990 words and 421878 word types\n",
      "PROGRESS: at sentence #200000, processed 1558051 words and 423290 word types\n",
      "PROGRESS: at sentence #201000, processed 1567101 words and 425243 word types\n",
      "PROGRESS: at sentence #202000, processed 1574887 words and 426881 word types\n",
      "PROGRESS: at sentence #203000, processed 1581697 words and 428334 word types\n",
      "PROGRESS: at sentence #204000, processed 1588663 words and 429798 word types\n",
      "PROGRESS: at sentence #205000, processed 1595380 words and 431089 word types\n",
      "PROGRESS: at sentence #206000, processed 1602417 words and 432546 word types\n",
      "PROGRESS: at sentence #207000, processed 1608880 words and 433790 word types\n",
      "collected 434328 token types (unigram + bigrams) from a corpus of 1611261 words and 207403 sentences\n",
      "merged Phrases<434328 vocab, min_count=10, threshold=0.5, max_vocab_size=40000000>\n",
      "Phrases lifecycle event {'msg': 'built Phrases<434328 vocab, min_count=10, threshold=0.5, max_vocab_size=40000000> in 2.57s', 'datetime': '2023-03-17T20:14:33.293357', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "exporting phrases from Phrases<434328 vocab, min_count=10, threshold=0.5, max_vocab_size=40000000>\n",
      "FrozenPhrases lifecycle event {'msg': 'exported FrozenPhrases<1131 phrases, min_count=10, threshold=0.5> from Phrases<434328 vocab, min_count=10, threshold=0.5, max_vocab_size=40000000> in 0.79s', 'datetime': '2023-03-17T20:14:34.080183', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n",
      "Backing off send_request(...) for 9.0s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000155D28A65E0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n",
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 74090 words, keeping 8337 word types\n",
      "PROGRESS: at sentence #20000, processed 148782 words, keeping 13209 word types\n",
      "PROGRESS: at sentence #30000, processed 224532 words, keeping 17247 word types\n",
      "PROGRESS: at sentence #40000, processed 303407 words, keeping 20845 word types\n",
      "PROGRESS: at sentence #50000, processed 379102 words, keeping 23663 word types\n",
      "PROGRESS: at sentence #60000, processed 451000 words, keeping 26190 word types\n",
      "PROGRESS: at sentence #70000, processed 525840 words, keeping 28934 word types\n",
      "PROGRESS: at sentence #80000, processed 600542 words, keeping 31717 word types\n",
      "PROGRESS: at sentence #90000, processed 678259 words, keeping 33970 word types\n",
      "PROGRESS: at sentence #100000, processed 756595 words, keeping 36134 word types\n",
      "PROGRESS: at sentence #110000, processed 835461 words, keeping 38550 word types\n",
      "PROGRESS: at sentence #120000, processed 920557 words, keeping 40835 word types\n",
      "PROGRESS: at sentence #130000, processed 1000212 words, keeping 42904 word types\n",
      "PROGRESS: at sentence #140000, processed 1075496 words, keeping 45071 word types\n",
      "PROGRESS: at sentence #150000, processed 1157229 words, keeping 47194 word types\n",
      "PROGRESS: at sentence #160000, processed 1235036 words, keeping 49137 word types\n",
      "PROGRESS: at sentence #170000, processed 1306568 words, keeping 51107 word types\n",
      "PROGRESS: at sentence #180000, processed 1380786 words, keeping 52898 word types\n",
      "PROGRESS: at sentence #190000, processed 1446965 words, keeping 54702 word types\n",
      "PROGRESS: at sentence #200000, processed 1517093 words, keeping 56602 word types\n",
      "collected 58021 word types from a corpus of 1569333 raw words and 207403 sentences\n",
      "Creating a fresh vocabulary\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 retains 11626 unique words (20.037572603023044%% of original 58021, drops 46395)', 'datetime': '2023-03-17T20:14:36.410864', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'effective_min_count=5 leaves 1506148 word corpus (95.9737671991859%% of original 1569333, drops 63185)', 'datetime': '2023-03-17T20:14:36.412789', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "deleting the raw counts dictionary of 58021 items\n",
      "sample=0.001 downsamples 48 most-common words\n",
      "Word2Vec lifecycle event {'msg': 'downsampling leaves estimated 1091871.2557188598 word corpus (72.5%% of prior 1506148)', 'datetime': '2023-03-17T20:14:36.513206', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'prepare_vocab'}\n",
      "estimated required memory for 11626 words and 150 dimensions: 19764200 bytes\n",
      "resetting layer weights\n",
      "Word2Vec lifecycle event {'update': False, 'trim_rule': 'None', 'datetime': '2023-03-17T20:14:36.689120', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'build_vocab'}\n",
      "Word2Vec lifecycle event {'msg': 'training model with 3 workers on 11626 vocabulary and 150 features, using sg=1 hs=0 sample=0.001 negative=5 window=5', 'datetime': '2023-03-17T20:14:36.690116', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "EPOCH 1 - PROGRESS: at 41.10% examples, 437606 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 1 - PROGRESS: at 82.14% examples, 449943 words/s, in_qsize 6, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 1569333 raw words (1091993 effective words) took 2.4s, 453412 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 38.51% examples, 403616 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 2 - PROGRESS: at 75.42% examples, 412979 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 1569333 raw words (1091313 effective words) took 2.6s, 424609 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 36.63% examples, 395910 words/s, in_qsize 6, out_qsize 0\n",
      "EPOCH 3 - PROGRESS: at 71.31% examples, 395767 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 1569333 raw words (1092060 effective words) took 2.8s, 396266 effective words/s\n",
      "Backing off send_request(...) for 45.8s (requests.exceptions.ConnectionError: HTTPSConnectionPool(host='api.segment.io', port=443): Max retries exceeded with url: /v1/batch (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x00000155D28A65B0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')))\n",
      "EPOCH 4 - PROGRESS: at 34.07% examples, 368158 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 4 - PROGRESS: at 69.56% examples, 383976 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 1569333 raw words (1091144 effective words) took 2.8s, 396212 effective words/s\n",
      "EPOCH 5 - PROGRESS: at 32.09% examples, 342839 words/s, in_qsize 5, out_qsize 0\n",
      "EPOCH 5 - PROGRESS: at 63.16% examples, 348572 words/s, in_qsize 6, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 1569333 raw words (1092060 effective words) took 2.9s, 375506 effective words/s\n",
      "Word2Vec lifecycle event {'msg': 'training on 7846665 raw words (5458570 effective words) took 13.5s, 405483 effective words/s', 'datetime': '2023-03-17T20:14:50.152765', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'train'}\n",
      "Word2Vec lifecycle event {'params': 'Word2Vec(vocab=11626, vector_size=150, alpha=0.025)', 'datetime': '2023-03-17T20:14:50.152765', 'gensim': '4.0.1', 'python': '3.9.1 (tags/v3.9.1:1e5d33e, Dec  7 2020, 17:08:21) [MSC v.1927 64 bit (AMD64)]', 'platform': 'Windows-10-10.0.19041-SP0', 'event': 'created'}\n"
     ]
    }
   ],
   "source": [
    "w2v_salvini = w2v(sentences, model='SG', vector_size=150, min_count=5, get_bigrams=True)\n",
    "w2v_salvini_vocab = get_w2v_vocab(w2v_salvini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('dittatura', 0.8013535737991333),\n",
       " ('fomentare', 0.7936769127845764),\n",
       " ('direttivo', 0.791358470916748),\n",
       " ('organizzazione', 0.771776556968689),\n",
       " ('soros', 0.7713298797607422),\n",
       " ('lobby', 0.7678341269493103),\n",
       " ('comunit√†_europeo', 0.7638823390007019),\n",
       " ('casto', 0.7624161839485168),\n",
       " ('toga', 0.7579337954521179),\n",
       " ('giornalaio', 0.7567523121833801),\n",
       " ('rovinare_dellitalia', 0.7558698058128357),\n",
       " ('multinazionale', 0.755838930606842),\n",
       " ('strategia', 0.7520187497138977),\n",
       " ('corrotto', 0.7516555786132812),\n",
       " ('burocrate', 0.7489603161811829),\n",
       " ('propriet√†_privato', 0.7483412027359009),\n",
       " ('europeista', 0.7471895813941956),\n",
       " ('legemonia', 0.7462050914764404),\n",
       " ('ricatto', 0.745956540107727),\n",
       " ('minaccia', 0.7453929781913757),\n",
       " ('gestione', 0.7453019022941589),\n",
       " ('raggio', 0.7446823716163635),\n",
       " ('complicit√†', 0.7434127330780029),\n",
       " ('massone', 0.7427975535392761),\n",
       " ('scomparire', 0.7418749928474426),\n",
       " ('maggioranza', 0.7407003045082092),\n",
       " ('acqua_territoriale', 0.7394383549690247),\n",
       " ('uscire_dallunione', 0.7391312122344971),\n",
       " ('dittatoriale', 0.738659679889679),\n",
       " ('cartabia', 0.7383782863616943),\n",
       " ('terrorismo', 0.7375129461288452),\n",
       " ('mangiatoia', 0.7365491390228271),\n",
       " ('spariscono', 0.7351173162460327),\n",
       " ('nuovo_segretaria', 0.7345322370529175),\n",
       " ('affossare', 0.73436439037323),\n",
       " ('piagare', 0.7342314124107361),\n",
       " ('sconfitto', 0.7341349124908447),\n",
       " ('imposizione', 0.7333918213844299),\n",
       " ('stragrande', 0.7324152588844299),\n",
       " ('sciacallo', 0.7322616577148438),\n",
       " ('logico', 0.7320414781570435),\n",
       " ('accusa', 0.7307461500167847),\n",
       " ('zecca_rosso', 0.7303705215454102),\n",
       " ('scandaloso', 0.7298684120178223),\n",
       " ('cooperativo', 0.7292966842651367),\n",
       " ('lassismo', 0.7287731170654297),\n",
       " ('fuori_dalleuropa', 0.7281039953231812),\n",
       " ('schiavo', 0.7277520298957825),\n",
       " ('segreteria', 0.7276787757873535),\n",
       " ('rave_party', 0.7271859645843506),\n",
       " ('protesta', 0.726367175579071),\n",
       " ('associazione', 0.7257294654846191),\n",
       " ('illegalit√†', 0.7255266308784485),\n",
       " ('virus', 0.7252029776573181),\n",
       " ('influenzare', 0.7247072458267212),\n",
       " ('criminalit√†_organizzato', 0.7241837382316589),\n",
       " ('taglione', 0.7240684032440186),\n",
       " ('organizzato', 0.723948061466217),\n",
       " ('schierare', 0.7235149145126343),\n",
       " ('istituzione', 0.7233536839485168),\n",
       " ('amministrazione', 0.7233012914657593),\n",
       " ('agenzia', 0.7221519947052002),\n",
       " ('sbagliato', 0.7218712568283081),\n",
       " ('segretaria', 0.7206941246986389),\n",
       " ('stampare', 0.7200205326080322),\n",
       " ('magistrato', 0.7192574739456177),\n",
       " ('lunione_europeo', 0.7186779379844666),\n",
       " ('femminista', 0.7186452150344849),\n",
       " ('emanare', 0.7183219194412231),\n",
       " ('malefatta', 0.7182633876800537),\n",
       " ('porcate', 0.7180613875389099),\n",
       " ('mentalit√†', 0.7177682518959045),\n",
       " ('ue', 0.7176898121833801),\n",
       " ('digos', 0.7176487445831299),\n",
       " ('applicazione', 0.7172470688819885),\n",
       " ('larte', 0.7165982723236084),\n",
       " ('convenienza', 0.7164591550827026),\n",
       " ('desto', 0.7163849472999573),\n",
       " ('linteresse', 0.7152351140975952),\n",
       " ('indagato', 0.7151545286178589),\n",
       " ('polizia_locale', 0.7144545316696167),\n",
       " ('contrastare', 0.7142671346664429),\n",
       " ('borseggiatore', 0.7136776447296143),\n",
       " ('sovranit√†', 0.7135815620422363),\n",
       " ('finto_buonista', 0.7131653428077698),\n",
       " ('dellimmigrazione', 0.7131190299987793),\n",
       " ('dellordine', 0.7129867076873779),\n",
       " ('scatenare', 0.7128078937530518),\n",
       " ('lgbt', 0.7120492458343506),\n",
       " ('ad_hoc', 0.7120046615600586),\n",
       " ('ecologia', 0.71192467212677),\n",
       " ('estremista', 0.7116507291793823),\n",
       " ('dellunione_europeo', 0.7115740180015564),\n",
       " ('cgil', 0.7107476592063904),\n",
       " ('laiuto', 0.7105260491371155),\n",
       " ('unione', 0.7097140550613403),\n",
       " ('kompagni', 0.709658145904541),\n",
       " ('prepotenza', 0.7096158266067505),\n",
       " ('opposto', 0.7095814943313599),\n",
       " ('malfattore', 0.7094588279724121)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wsim(w2v_salvini, 'magistratura', topn=100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
